import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.metrics import roc_curve, roc_auc_score
import numpy as np
import matplotlib.pyplot as plt
import csv
from collections import Counter


def get_best_params(X_train_val, y_train_val, nb_splits, scorer, param_grid, estimator):
    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=nb_splits, scoring=scorer)
    grid_search.fit(X_train_val, y_train_val)
    return {'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'best_estimator': grid_search.best_estimator_,
            'cv_results': pd.DataFrame(grid_search.cv_results_)}


def get_best_LinearSVC_param(X_train_val, y_train_val, nb_splits=10, scorer='roc_auc',
                             c_range=np.logspace(-5, 2, num=100)):
    grid_search_dict = get_best_params(X_train_val, y_train_val,
                                       nb_splits, scorer,
                                       {'c': c_range}, LinearSVC())

    cv_data_frame = grid_search_dict['cv_results']
    grid_search_dict['cv_results'] = pd.DataFrame({'param_c': cv_data_frame['param_c'],
                                                   'mean_test_score': cv_data_frame['mean_test_score'],
                                                   'rank_test_score': cv_data_frame['rank_test_score']})
    return grid_search_dict


def get_features_weights(linearSVC: 'LinearSVC', vector_space_dict: dict) -> dict:
    coefs = linearSVC.coef_
    features = np.sort(vector_space_dict.keys())
    feature_weight_dict = {feature: coef for feature, coef in zip(features, coefs)}
    feature_weight_dict['_intercept'] = linearSVC.intercept_

    return feature_weight_dict


def get_features_weights_csv(linearSVC: 'LinearSVC', vector_space_dict: dict, csv_name='feature_weight.csv'):
    with open(csv_name, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(('feature', 'weight'))
        rows = [(feature, weight) for feature, weight in get_features_weights(linearSVC, vector_space_dict)]
        writer.writerows(rows)


def show_plot(x_values, y_values, line_label, x_label, y_label):
    show_plots([x_values], [y_values], [line_label], ['r'], x_label, y_label)


def show_plots(x_values, y_values, line_labels, colors, x_label, y_label):
    for x_value, y_value, line_label, color in zip(x_values, y_values, line_labels, colors):
        plt.plot(x_value, y_value, color, label=line_label)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend().draggable()
    plt.show()


def show_linearSVC_roc_curv(linearSVC: 'LinearSVC', X_test, y_test, line_label):
    fprs, tprs, _ = roc_curve(y_test, linearSVC.decision_function(X_test))
    auc_score = roc_auc_score(y_test, linearSVC.decision_function(X_test))
    show_plot(fprs, tprs,
              line_label + ', AUC= ' + str(auc_score),
              'FPR', 'TPR')


def show_linearSVC_class_separation(linearSVC: 'LinearSVC', X_test, y_test):
    y_decision_score = linearSVC.decision_function(X_test)
    y_positive_decision_score = y_decision_score[y_test == 1]
    y_negative_decision_score = y_decision_score[y_test == 0]
    positive_count = Counter(y_positive_decision_score)
    negative_count = Counter(y_negative_decision_score)
    y_positive_decision_score = list(positive_count.keys())
    y_positive_distribution = list(positive_count.values())
    y_negative_decision_score = list(negative_count.keys())
    y_negative_distribution = list(negative_count.values())
    plt.fill_between(y_positive_decision_score, 0, y_positive_distribution, color='blue', alpha=0.5, hatch='.')
    plt.fill_between(y_negative_decision_score, 0, y_negative_distribution, color='red', alpha=0.5, hatch='.')
    plt.legend(['true_positives', 'true_negatives']).draggable()
    plt.xlabel('svm decision_function values')
    plt.ylabel('number of data points')


