import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from dimension_manager import get_top_usage_ratio_dimensions
import itertools
from vector_space_construction import get_vector_space_dict, get_X_y


def get_best_params(X_train_val, y_train_val, nb_splits, scorer, param_grid, estimator):
    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=nb_splits, scoring=scorer)
    grid_search.fit(X_train_val, y_train_val)
    return {'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'best_estimator': grid_search.best_estimator_,
            'cv_results': pd.DataFrame(grid_search.cv_results_)}


def get_best_LinearSVC_param(X_train_val, y_train_val, nb_splits=10, scorer='roc_auc',
                             c_range=np.logspace(-5, 2, num=100)):
    grid_search_dict = get_best_params(X_train_val, y_train_val,
                                       nb_splits, scorer,
                                       {'C': c_range}, LinearSVC())

    cv_data_frame = grid_search_dict['cv_results']
    grid_search_dict['cv_results'] = pd.DataFrame({'param_C': cv_data_frame['param_C'],
                                                   'mean_test_score': cv_data_frame['mean_test_score'],
                                                   'rank_test_score': cv_data_frame['rank_test_score']})
    return grid_search_dict


def get_best_top_usage_ratio_LinearSVC_param(apks_train_val, nb_splits=10, scorer='roc_auc',
                                             c_range=np.logspace(-5, 2, num=100),
                                             limit_range=np.arange(100, 1001, 100),
                                             malware_usage_range=np.linspace(0.1, 1, 10),
                                             malware_ratio_range=np.linspace(0.1, 1, 10)):

    return get_best_reduced_dimensions_LinearSVC_param(apks_train_val=apks_train_val,
                                                       dimension_retriever=get_top_usage_ratio_dimensions,
                                                       nb_splits=nb_splits, scorer=scorer, c_range=c_range,
                                                       limit_range=limit_range, malware_ratio_range=malware_ratio_range,
                                                       malware_usage_range=malware_usage_range)


def get_best_reduced_dimensions_LinearSVC_param(apks_train_val, dimension_retriever,
                                                nb_splits=10, scorer='roc_auc',
                                                c_range=np.logspace(-5, 2, num=100),
                                                limit_range=np.arange(100, 1001, 100),
                                                malware_usage_range=np.linspace(0.1, 1, 10),
                                                malware_ratio_range=np.linspace(0.1, 1, 10)):
    best_grid_search_result = {
        'best_params': None,
        'best_score': 0,
        'best_estimator': None,
        'cv_results': None
    }
    for limit, malware_usage, malware_ratio in itertools.product(limit_range,
                                                                 malware_usage_range, malware_ratio_range):

        dimensions = dimension_retriever(limit=limit, malware_usage=malware_usage,
                                         malware_ratio=malware_ratio)

        vector_space_dict = get_vector_space_dict(dimensions)
        X_train_val, y_train_val = get_X_y(vector_space_dict, apks_train_val)

        grid_search_result = get_best_LinearSVC_param(X_train_val, y_train_val, nb_splits=nb_splits,
                                                      scorer=scorer, c_range=c_range)

        if  grid_search_result['best_score'] > best_grid_search_result['best_score']:
            best_grid_search_result = grid_search_result
            best_grid_search_result['best_params'] = {'C': best_grid_search_result['best_params']['C'],
                                                      'limit': limit,
                                                      'malware_usage': malware_usage,
                                                      'malware_ratio': malware_ratio}
    return best_grid_search_result


def show_plot(x_values, y_values, line_label, x_label, y_label):
    show_plots([x_values], [y_values], [line_label], ['r'], x_label, y_label)


def show_plots(x_values, y_values, line_labels, colors, x_label, y_label):
    for x_value, y_value, line_label, color in zip(x_values, y_values, line_labels, colors):
        plt.plot(x_value, y_value, color, label=line_label)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend().draggable()
    plt.show()


def show_linearSVC_roc_curve(linearSVC: 'LinearSVC', X_test, y_test, line_label):
    fprs, tprs, _ = roc_curve(y_test, linearSVC.decision_function(X_test))
    auc_score = roc_auc_score(y_test, linearSVC.decision_function(X_test))
    show_plot(fprs, tprs,
              line_label + ', AUC= ' + str(auc_score),
              'FPR', 'TPR')


def show_linearSVC_class_separation(linearSVC: 'LinearSVC', X_test, y_test):
    y_decision_score = linearSVC.decision_function(X_test)
    y_positive_decision_score = y_decision_score[y_test == 1]
    y_negative_decision_score = y_decision_score[y_test == 0]
    positive_count = Counter(y_positive_decision_score)
    negative_count = Counter(y_negative_decision_score)
    y_positive_decision_score = list(positive_count.keys())
    y_positive_distribution = list(positive_count.values())
    y_negative_decision_score = list(negative_count.keys())
    y_negative_distribution = list(negative_count.values())
    plt.fill_between(y_positive_decision_score, 0, y_positive_distribution, color='blue', alpha=0.5, hatch='')
    plt.plot(y_positive_decision_score, y_positive_distribution, color='blue', marker='.')
    plt.fill_between(y_negative_decision_score, 0, y_negative_distribution, color='red', alpha=0.5, hatch='')
    plt.plot(y_negative_decision_score, y_negative_distribution, color='red', marker='.')
    plt.legend(['true_positives', 'true_negatives']).draggable()
    plt.xlabel('svm decision_function values')
    plt.ylabel('number of data points')
    plt.show()


def get_linearSVC_validation_report(linearSVC: 'LinearSVC', X_test, y_test):
    auc = roc_auc_score(y_test, linearSVC.decision_function(X_test))
    conf_matrix = confusion_matrix(y_test, linearSVC.predict(X_test), labels=[0, 1])
    tn = conf_matrix[0, 0]
    tp = conf_matrix[1, 1]
    fp = conf_matrix[0, 1]
    fn = conf_matrix[1, 0]
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    recall = tp / (tp + fn)
    precision = tp / (tp + fp)
    f1 = 2 * (precision * recall) / (precision + recall)
    return {
        'auc': auc,
        'accuracy': accuracy,
        'recall': recall,
        'precision': precision,
        'tp': tp,
        'tn': tn,
        'fp': fp,
        'fn': fn
    }
